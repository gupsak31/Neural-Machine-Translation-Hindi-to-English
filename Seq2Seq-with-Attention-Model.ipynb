{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS779: Seq2Seq with Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-e5SnzdvjZu"
      },
      "source": [
        "# **References**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Reference Papers:\n",
        "\n",
        "\n",
        "*   Sequence to Sequence Learning with Neural Networks [[link](https://arxiv.org/abs/1409.3215)]\n",
        "*   iNLTK: Natural Language Toolkit for Indic Languages [[link](https://arxiv.org/abs/2009.12534)]\n",
        "*   Learning Phrase Representations using RNN Encoder-Decoder [[link](https://arxiv.org/abs/1406.1078)]\n",
        "*   \"A Passage to India\": Pre-trained Word Embeddings for Indian Languages [[link](https://www.aclweb.org/anthology/2020.sltu-1.49.pdf)]\n",
        "*   Neural Machine Translation by Jointly Learning to Align and Translate [[link](https://arxiv.org/abs/1409.0473)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Reference Tutorials:\n",
        "\n",
        "*   How to use Pre-trained Word Embeddings in PyTorch [[link](https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76)]\n",
        "*   Mechanics of Seq2seq Models With Attention [[link](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Reference for Code: [Pytorch Tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHd_-CtMwWj4"
      },
      "source": [
        "# **Setup**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Connect with Gdrive.\n",
        "*   Download and load libraries/packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HksS36SJ3hXZ"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UKN_token = 2\n",
        "MAX_LENGTH = 500\n",
        "teacher_forcing_ratio = 1\n",
        "N_Epochs = 3\n",
        "NUM_LAYERS = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMgC3k160Dof",
        "outputId": "2a48fa08-9693-4af7-b0e7-48443c1731c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5uBZZ6f0Dlr",
        "outputId": "72db4537-7cba-4516-890b-bc5e1c12d98d"
      },
      "source": [
        "%cd gdrive/MyDrive/CS779:\\ Competition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/CS779: Competition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSmmWUnS0JqN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQX7YYbnt8Vi"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkGxUaysuQZP",
        "outputId": "70964ea7-6250-4137-ece3-4fed9c5dc69e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on =\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on = cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OXEOGlDzwbF"
      },
      "source": [
        "Setup SpaCy\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nszHbSpstKjN"
      },
      "source": [
        "!python3 -m spacy download en >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bELTiiLftMV8"
      },
      "source": [
        "import spacy\n",
        "eng = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XFDcKexyHAn"
      },
      "source": [
        "Setup indicnlp library\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFcugN_1tVws"
      },
      "source": [
        "#### Ony run to download the indic library (first time only) \n",
        "# !git clone https://github.com/anoopkunchukuttan/indic_nlp_library >> /dev/null\n",
        "# !git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxLSjiPetXl-"
      },
      "source": [
        "!pip install Morfessor >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuRjrZf1tcVV"
      },
      "source": [
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME = \"indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES = \"indic_nlp_resources\"\n",
        "\n",
        "# Add Library to Python path\n",
        "import sys\n",
        "sys.path.append(r\"{}\".format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "# Set environment variable\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "# Initialize the Indic NLP library\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQCuovAqtev5"
      },
      "source": [
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msNAQDKtyWRT"
      },
      "source": [
        "Setup iNLTK library\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOeGbX_JthpY"
      },
      "source": [
        "!pip install inltk >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k73OUxuHthmk",
        "outputId": "9cb65969-bb74-4c09-cb1e-c5414f42a0a6"
      },
      "source": [
        "from inltk.inltk import setup\n",
        "'''\n",
        "Note: Run setup('<code-of-language>') to a language for the FIRST TIME ONLY.\n",
        "This will download all the necessary models required to do inference for that language.\n",
        "'''\n",
        "try:\n",
        "    setup(\"hi\")\n",
        "except:\n",
        "    print(\"Downloading 'hindi' setup... Wait before running next cell!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 'hindi' setup... Wait before running next cell!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwW51GUptlB3"
      },
      "source": [
        "from inltk.inltk import tokenize as inltk_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65GV926stWOS"
      },
      "source": [
        "Setup NLTK library - For evaluation\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_St0i1YtWwG"
      },
      "source": [
        "!pip install -U nltk >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87TPIP4dtnv_",
        "outputId": "dbdff160-7c6d-4cee-d769-703e898386de"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAM3qEGqwnHn"
      },
      "source": [
        "# **Load Data**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Load training data\n",
        "*   Load Embedding matrix, word2index and index2word for Hindi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "6mfhSEtO2n7y",
        "outputId": "ebbb83bd-5075-4472-91ba-4822ea015a19"
      },
      "source": [
        "df = pd.read_csv(\"train/train.csv\")\n",
        "df = df[[\"hindi\", \"english\"]]\n",
        "display(df.head(5))\n",
        "print(\"\\nTotal hindi-english sentence pairs = \", df.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...</td>\n",
              "      <td>In El Salvador, both sides that withdrew from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>मैं उनके साथ कोई लेना देना नहीं है.</td>\n",
              "      <td>I have nothing to do with them.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-हटाओ रिक.</td>\n",
              "      <td>Fuck them, Rick.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>क्योंकि यह एक खुशियों भरी फ़िल्म है.</td>\n",
              "      <td>Because it's a happy film.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               hindi                                            english\n",
              "0  एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...  In El Salvador, both sides that withdrew from ...\n",
              "1                मैं उनके साथ कोई लेना देना नहीं है.                    I have nothing to do with them.\n",
              "2                                         -हटाओ रिक.                                   Fuck them, Rick.\n",
              "3               क्योंकि यह एक खुशियों भरी फ़िल्म है.                         Because it's a happy film.\n",
              "4                   The thought reaching the eyes...                   The thought reaching the eyes..."
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total hindi-english sentence pairs =  102322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WXmzA0F2six"
      },
      "source": [
        "# #####################################################################\n",
        "# #### Testing on smaller data\n",
        "# #####################################################################\n",
        "# df = df[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcINzJrH0T-z"
      },
      "source": [
        "my_embeddings = np.load(\"hindi_embeddings.npy\")\n",
        "word2index_npy = np.load(\"word2index.npy\")\n",
        "index2word_npy = np.load(\"index2word.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a2bgOFnkjN5"
      },
      "source": [
        "hindi_word2index = {}\n",
        "hindi_index2word = {}\n",
        "\n",
        "for i in range(len(word2index_npy)):\n",
        "    hindi_word2index[word2index_npy[i, 0]] = int(word2index_npy[i, 1])\n",
        "\n",
        "for i in range(len(index2word_npy)):\n",
        "    hindi_index2word[int(index2word_npy[i, 0])] = index2word_npy[i, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-aLgzg16qq"
      },
      "source": [
        "# **Data Normalisation and Pre-processing**\n",
        "\n",
        "---\n",
        "\n",
        "*   Remove nuktas from Hindi sentences.\n",
        "*   Create **embedding matrix** using iNLTK library.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab8qyHVcBFBX",
        "outputId": "077b883d-84af-45e1-804f-ce7fd4eecb63"
      },
      "source": [
        "factory = IndicNormalizerFactory()\n",
        "normalizer = factory.get_normalizer(\"hi\", remove_nuktas = True)\n",
        "\n",
        "for i, pair in df.iterrows():\n",
        "    df.iloc[i, 0] = normalizer.normalize(pair[0])\n",
        "\n",
        "    if(i != 0 and i%10000 == 0):\n",
        "        print(\"Iterations done =\", i)\n",
        "print(\"...Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations done = 10000\n",
            "Iterations done = 20000\n",
            "Iterations done = 30000\n",
            "Iterations done = 40000\n",
            "Iterations done = 50000\n",
            "Iterations done = 60000\n",
            "Iterations done = 70000\n",
            "Iterations done = 80000\n",
            "Iterations done = 90000\n",
            "Iterations done = 100000\n",
            "...Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l9o8QUr0JnS"
      },
      "source": [
        "class Language:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        if (name == \"english\"):\n",
        "            self.n_words = 3  # UKN, SOS and EOS\n",
        "            self.word2index = {\"<sos>\":0, \"<eos>\":1, \"<ukn>\":2}\n",
        "            self.index2word = {0: \"<sos>\", 1: \"<eos>\", 2: \"<ukn>\"}\n",
        "        elif (name == \"hindi\"):\n",
        "            self.n_words = len(hindi_word2index)\n",
        "            self.word2index = hindi_word2index\n",
        "            self.index2word = hindi_index2word\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        if (self.name == \"english\"):\n",
        "            tokens = eng(sentence)\n",
        "            for token in tokens:\n",
        "                self.addWord(token.text)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKZfyzZy0Jlq"
      },
      "source": [
        "def readData():\n",
        "    print(\"Reading data...\")\n",
        "    pairs = [[sent for sent in pair] for _, pair in df.iterrows()]\n",
        "    input_lang = Language(\"hindi\")\n",
        "    output_lang = Language(\"english\")\n",
        "\n",
        "    # print(\"Data normalisation...\")\n",
        "    #############################################################################\n",
        "    ### Add normalising techniques\n",
        "    #############################################################################\n",
        "\n",
        "    print(\"Data preprocessing...\")\n",
        "    for i, pair in enumerate(pairs):\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "        if(i != 0 and i%10000 == 0):\n",
        "            print(\"Iterations done = \", i)\n",
        "    print(\"... Done\")\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsmkHj_40Jjp",
        "outputId": "c17c2463-e48f-473d-bc58-03f32f9165f2"
      },
      "source": [
        "input_lang, output_lang, pairs = readData()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data...\n",
            "Data preprocessing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZNx1pQvTjYE"
      },
      "source": [
        "# **Utility Functions**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv3BnYRiVDlC"
      },
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base = 0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czgIlvyzd1gK"
      },
      "source": [
        "The below cell has utility functions to return a pair of input-ouput tensor.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOBF0LEO-oGd"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    idx_list = []\n",
        "    if (lang.name == \"english\"):\n",
        "        tokens = eng(sentence)\n",
        "        for token in tokens:\n",
        "            if token.text in lang.word2index:\n",
        "                idx_list.append(lang.word2index[token.text])\n",
        "            else:\n",
        "                idx_list.append(UKN_token)\n",
        "    else:\n",
        "        tokens = inltk_tokenize(sentence, \"hi\")\n",
        "        for token in tokens:\n",
        "                if token in lang.word2index:\n",
        "                    idx_list.append(lang.word2index[token])\n",
        "                else:\n",
        "                    idx_list.append(UKN_token)\n",
        "    idx_list.append(EOS_token)\n",
        "    return idx_list\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    return torch.tensor(indexes, dtype = torch.long, device = device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    return (tensorFromSentence(input_lang, pair[0]), tensorFromSentence(output_lang, pair[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gjXKNPoey3a"
      },
      "source": [
        "Split the training data into Train-Validation split. Half the validation data is used as Test data.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyTpeuQI0gzd"
      },
      "source": [
        "#Temporary data\n",
        "data = np.reshape(np.random.randn(2*df.shape[0]),(df.shape[0], 2))\n",
        "labels = np.random.randint(2, size = df.shape[0])\n",
        "X = pd.DataFrame(data, columns = ['Column_1', 'Column_2'])\n",
        "y = pd.Series(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIL_Vii1HRUi"
      },
      "source": [
        "train_indexes = list(X_train.index)\n",
        "test_indexes = list(X_test.index)\n",
        "\n",
        "test_df = df.iloc[test_indexes]\n",
        "train_df = df.iloc[train_indexes]\n",
        "\n",
        "### The below lines \n",
        "# validation_pairs = [tensorsFromPair(list(test_df.iloc[i])) for i in range(test_df.shape[0])]\n",
        "# train_pairs = [[sent for sent in pair] for _, pair in train_df.iterrows()]\n",
        "# train_pairs = [tensorsFromPair(train_pairs[idx]) for idx in range(len(train_pairs))]\n",
        "\n",
        "validation_pairs = torch.load(\"val.pt\")\n",
        "test_pairs = validation_pairs[int(len(validation_pairs)/2):]\n",
        "validation_pairs = validation_pairs[:int(len(validation_pairs)/2)]\n",
        "train_pairs = torch.load(\"train.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3PAJil-gerq"
      },
      "source": [
        "def inputData(n_iter):\n",
        "    idx = int((n_iter - 1) - train_df.shape[0]*int((n_iter - 1)/train_df.shape[0]))\n",
        "    train = train_pairs[idx]\n",
        "    return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNo9Ucg24Qo9"
      },
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience = 10, verbose = True):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, val_loss, encoder, decoder):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, encoder, decoder)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, encoder, decoder)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, encoder, decoder):\n",
        "        '''Saves model when loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.5f} --> {val_loss:.5f}).  Saving model ...\\n\")\n",
        "        torch.save(encoder.state_dict(), \"encoder_bestVal.pt\")\n",
        "        torch.save(decoder.state_dict(), \"decoder_bestVal.pt\")\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GWoGneGU5oD"
      },
      "source": [
        "# **Model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBfHWOiSBQB5"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, input_vocab = input_lang.n_words, emb_size = 400):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_vocab, emb_size)\n",
        "        self.embedding.from_pretrained(torch.from_numpy(my_embeddings), freeze = True)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLjZrM6oBP_k"
      },
      "source": [
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, emb_size = 400, output_vocab = output_lang.n_words, max_length = MAX_LENGTH):\n",
        "        super(AttnDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_vocab = output_vocab\n",
        "        self.max_length = max_length\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_vocab, self.emb_size)\n",
        "        self.attn = nn.Linear((self.hidden_size + self.emb_size), self.max_length)\n",
        "        self.attn_combine = nn.Linear((self.hidden_size + self.emb_size), self.hidden_size)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_vocab)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "\n",
        "        attn_weights = self.attn(torch.cat((embedded[0], hidden[0]), dim = 1))\n",
        "        attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), dim = 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = F.log_softmax(self.out(output[0]), dim = 1)\n",
        "\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMu0HL1gVvKR"
      },
      "source": [
        "# **Train**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3iojfyH_2ct"
      },
      "source": [
        "LOSS_FXN = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-22hz_rBP8_"
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_fxn, max_length = MAX_LENGTH):\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
        "\n",
        "    loss = 0\n",
        "    for ei in range(min(input_length, max_length)):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss +=  loss_fxn(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss +=  loss_fxn(decoder_output, target_tensor[di])\n",
        "\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item()/target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pks7O64thOm8"
      },
      "source": [
        "def validate(validation_pairs, encoder, decoder, loss_fxn, max_length = MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "\n",
        "        for i in range(len(validation_pairs)):\n",
        "            input_tensor = validation_pairs[i][0]\n",
        "            target_tensor = validation_pairs[i][1]\n",
        "\n",
        "            input_length = input_tensor.size(0)\n",
        "            target_length = target_tensor.size(0)\n",
        "\n",
        "            encoder_hidden = encoder.initHidden()\n",
        "            encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
        "\n",
        "            loss = 0\n",
        "            for ei in range(min(input_length, max_length)):\n",
        "                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "                encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "            decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze().detach()\n",
        "\n",
        "                loss +=  loss_fxn(decoder_output, target_tensor[di])\n",
        "                if decoder_input.item() == EOS_token:\n",
        "                    break\n",
        "\n",
        "        val_loss += loss.item()/target_length\n",
        "\n",
        "    return val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYYxPXGzLo6-"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters = 0, print_every = 1000, plot_every = 100, val_every = 1, learning_rate = 0.01):\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "    min_loss = np.Inf\n",
        "    plot_losses = []\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
        "\n",
        "    loss_fxn = LOSS_FXN\n",
        "    early_stopping = EarlyStopping(patience = 3, verbose = True)\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = inputData(iter)\n",
        "\n",
        "        input_tensor = training_pair[0].to(device)\n",
        "        target_tensor = training_pair[1].to(device)\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_fxn)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if (iter%print_every) == 0:\n",
        "            print_loss_avg = print_loss_total/print_every\n",
        "            print_loss_total = 0\n",
        "            print(\"Iterations: %d (%d%%)\\nLoss = %.4f\\n\" %(iter, iter/n_iters*100, print_loss_avg))\n",
        "\n",
        "            if (print_loss_avg <= min_loss):\n",
        "                min_loss = print_loss_avg\n",
        "                torch.save(encoder.state_dict(), \"encoder_bestTrain.pt\")\n",
        "                torch.save(decoder.state_dict(), \"decoder_bestTrain.pt\")\n",
        "\n",
        "        if (iter%plot_every) == 0:\n",
        "            plot_loss_avg = plot_loss_total/plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "        if (iter%val_every) == 0:\n",
        "            val_loss = validate(validation_pairs, encoder, decoder, loss_fxn)\n",
        "            print(\"Validate loss = \", val_loss)\n",
        "            early_stopping(val_loss, encoder, decoder)        \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping...\")\n",
        "                break\n",
        "        \n",
        "    torch.save(encoder.state_dict(), \"encoder_final.pt\")\n",
        "    torch.save(decoder.state_dict(), \"decoder_final.pt\")\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPk8x4jJWN9C"
      },
      "source": [
        "hidden_size = 512\n",
        "encoder = Encoder(hidden_size).to(device)\n",
        "decoder = AttnDecoder(hidden_size).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "svlEXD3IBkD9",
        "outputId": "7823bb72-5ea9-4e52-f38b-7d9370b5c67f"
      },
      "source": [
        "trainIters(encoder, decoder, n_iters = N_Epochs*train_df.shape[0], print_every = 10000, plot_every = 5000, val_every = 50000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations: 10000 (3%)\n",
            "Loss = 5.1993\n",
            "\n",
            "Iterations: 20000 (7%)\n",
            "Loss = 4.4842\n",
            "\n",
            "Iterations: 30000 (10%)\n",
            "Loss = 4.1746\n",
            "\n",
            "Iterations: 40000 (14%)\n",
            "Loss = 4.0153\n",
            "\n",
            "Iterations: 50000 (18%)\n",
            "Loss = 3.8777\n",
            "\n",
            "Validate loss =  4.584026336669922\n",
            "Validation loss decreased (inf --> 4.58403).  Saving model ...\n",
            "\n",
            "Iterations: 60000 (21%)\n",
            "Loss = 3.7739\n",
            "\n",
            "Iterations: 70000 (25%)\n",
            "Loss = 3.7603\n",
            "\n",
            "Iterations: 80000 (28%)\n",
            "Loss = 3.6980\n",
            "\n",
            "Iterations: 90000 (32%)\n",
            "Loss = 3.6663\n",
            "\n",
            "Iterations: 100000 (36%)\n",
            "Loss = 3.5575\n",
            "\n",
            "Validate loss =  7.636861801147461\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Iterations: 110000 (39%)\n",
            "Loss = 3.5435\n",
            "\n",
            "Iterations: 120000 (43%)\n",
            "Loss = 3.4372\n",
            "\n",
            "Iterations: 130000 (47%)\n",
            "Loss = 3.3904\n",
            "\n",
            "Iterations: 140000 (50%)\n",
            "Loss = 3.3573\n",
            "\n",
            "Iterations: 150000 (54%)\n",
            "Loss = 3.2812\n",
            "\n",
            "Validate loss =  6.589169502258301\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Iterations: 160000 (57%)\n",
            "Loss = 3.3056\n",
            "\n",
            "Iterations: 170000 (61%)\n",
            "Loss = 3.2915\n",
            "\n",
            "Iterations: 180000 (65%)\n",
            "Loss = 3.2941\n",
            "\n",
            "Iterations: 190000 (68%)\n",
            "Loss = 3.2661\n",
            "\n",
            "Iterations: 200000 (72%)\n",
            "Loss = 3.2453\n",
            "\n",
            "Validate loss =  5.761155128479004\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhdZbn38e+dOWnGNumUpC2daEvpXChQxoICInAEERAFZDgHEQcUBT2H16PnvL6MiuKEghRREKQoIiBlqAyllJSOdJ6bdG6TdEgz3+8fewdDSNI0XcnO3vv3ua5e2cPq2r+sq7m78qxnPbe5OyIiEv0SIh1ARESCoYIuIhIjVNBFRGKECrqISIxQQRcRiRFJkfrg/Px8HzJkSKQ+XkQkKi1YsGC3uxe09l7ECvqQIUMoKSmJ1MeLiEQlM9vU1nsachERiREq6CIiMaJDQy5mthHYDzQA9e4+pZVtzgB+AiQDu9399OBiiojI4RzJGPqZ7r67tTfMLBf4BXCuu282s76BpBMRkQ4LasjlSmCWu28GcPedAe1XREQ6qKMF3YGXzWyBmd3YyvsjgTwzmxPe5out7cTMbjSzEjMr2bVrV2czi4hIKzo65DLd3cvCQymzzWylu7/RYj+TgRlAOvCOmc1z99XNd+LuDwEPAUyZMkXLPIqIBKhDZ+juXhb+uhN4FjihxSalwD/c/WB4nP0NYHyQQZus2r6fu19aSWVVXVfsXkQkah22oJtZLzPLanoMfAJY1mKzvwLTzSzJzDKAE4EVQYcF2Ly3il/MWcfGPQe7YvciIlGrI0Mu/YBnzaxp+z+6+0tm9h8A7v4rd19hZi8BS4BG4Lfu3rLoB6IoLx2A0vJDjC/O7YqPEBGJSoct6O6+nlaGT9z9Vy2e3wPcE1y01hWGC/qW8qqu/igRkagSdXeKZqclk5OeTKkKuojIR0RdQQco7p1OafmhSMcQEelRorKgF+VmqKCLiLQQnQU9L53S8ircNZVdRKRJVBb04t4ZVNc1svtAbaSjiIj0GFFZ0P81dVEXRkVEmkRpQc8AYIvG0UVEPhSlBV1n6CIiLXWooJvZRjNbamaLzKzNRqBmNtXM6s3s0uAiflyv1CR690rRTBcRkWYCaXABYGaJwF3Ay0edqgNCM11U0EVEmgQ55HIL8AzQLc0tivLSKd2rIRcRkSaBNLgws0Lg34BftreTIBtcFOdlUFpxiMZGzUUXEYGOF/Tp7j4JOA+42cxOa/H+T4DvuHtjeztx94fcfYq7TykoKOhE3H8pykuntr6R3Qdqjmo/IiKxokNj6M0bXJhZU4OL5h2LpgBPhpfYzQfON7N6d/9LwHk/9K+pi1X0zU7rqo8REYkagTS4cPdj3H2Iuw8B/gx8uSuLOXx0XXQREQmowUUX5mtT0xm6CrqISEhgDS6avX7N0cc6vPSURPIzU3RzkYhIWFTeKdqkMC+DLXt1hi4iAlFe0IvDy+iKiEiUF/SivAzKNBddRASI+oKeTl2Ds2N/daSjiIhEXNQXdNBMFxERiPKCXty7aeqixtFFRKK6oBfmhs/QNdNFRCS6C3paciIFWals0Rm6iEgwDS7M7PNmtiS8zVwz+9iNSF1F66KLiIQE1eBiA3C6u5eb2XnAQ8CJR52uA4rzMli0paI7PkpEpEcLZMjF3ee6e3n46TygKIj9dkRRXjpbKw7RoLnoIhLnAmlw0cJ1wIutvRFkg4smRXkZ1Dc62/dpLrqIxLegGlwAYGZnEiro32nt/SAbXDQp7t0000UXRkUkvnWooDdvcAE0Nbj4CDMbB/wWuMjd9wQZsj1aRldEJCSQBhdmNgiYBXzB3Vd3RdC2DMwNdSvS1EURiXdBNbi4E+gD/CK8Xb27T+mayB+VmpRIv+xUnaGLSNwLpMGFu18PXB9stI4rzsvQ7f8iEvei+k7RJkV56Wp0ISJxL0YKegbb91VT39AY6SgiIhETEwW9uHc6DY3OtkrNRReR+BUTBV1TF0VEYqagh24u0tRFEYlnMVHQB+SkY6YzdBGJbzFR0FOSEhiQnaapiyIS12KioENoHF2di0QkngXV4MLM7Kdmtjbc6GJS8FHbF2p0oTN0EYlfQTW4OA8YEf5zIvBLuqnBRZOi3hlsX1RGbX0jKUkx84uHiEiHBVX5LgIe85B5QK6ZDQho3x1SlJdOo8N2zUUXkTgVVIOLQmBLs+el4dc+oisaXDTR1EURiXeBNrg4nK5ocNGk+MObi1TQRSQ+BdXgogwobva8KPxatxmQk0ZigmkuuojErUAaXADPAV8Mz3aZBlS6+7bA07YjKTGB/tlpbFErOhGJU0E1uHgBOB9YC1QB13ZN3PaFpi7qDF1E4lNQDS4cuDnYaEeuuHcGb61pa2aliEhsi6kJ20V56ezYX01NfUOko4iIdLsYK+gZuMPWCs1FF5H4E2MFPTQXXVMXRSQexVRBL+6tRhciEr9iqqD3y0olKcE0dVFE4lJMFfSkxAQG5KbpDF1E4lJMFXQILQGgMXQRiUcxV9CL8tLZojN0EYlDHS7oZpZoZgvN7PlW3htkZq+H319iZucHG7PjxhbmsGt/DUtLKyMVQUQkIo7kDP1rwIo23vtP4Cl3nwhcDvziaIN11kUTCklPTuTxeZsiFUFEJCI62oKuCPgU8Ns2NnEgO/w4B9h69NE6Jyc9mYsmDOSvi8uorKqLVAwRkW7X0TP0nwDfBhrbeP/7wFVmVkpooa5bWtuoKxtcNHfVtMFU1zXyzPulXfYZIiI9TUeWz70A2OnuC9rZ7ArgUXcvIrTq4u/N7GP77soGF82NLcxh4qBcHp+3idC6YSIisa8jZ+inABea2UbgSeAsM3u8xTbXAU8BuPs7QBqQH2DOI/aFaYNZv/sgc9ftiWQMEZFuc9iC7u53uHuRuw8hdMHzNXe/qsVmm4EZAGY2mlBB77oxlQ44//gB5GUk8/t3dHFUROJDp+ehm9kPzOzC8NNvAjeY2WLgCeAaj/BYR1pyIpdNKWb2ih1sr9TqiyIS+46ooLv7HHe/IPz4Tnd/Lvx4ubuf4u7j3X2Cu7/cFWGP1JUnDqLRnSfmb450FBGRLhdzd4o2N7hPL04fWcAT8zdT19DWBB0RkdgQ0wUdQhdHd+6vYfbyHZGOIiLSpWK+oJ9xbF8Kc9N156iIxLyYL+iJCcaVJw5i7ro9rN25P9JxRES6TMwXdIDPTS0mOdF4fJ4ujopI7IqLgp6fmcr5xw/gmQWlVNXWRzqOiEiXiIuCDqH1XfbX1PPcooitGyYi0qXipqBPGZzHqP5ZPPaO1ncRkdgUSIOL8PuXmdlyM/vAzP4YXMRgmBlXTRvM8m37WLilItJxREQCF0iDCzMbAdwBnOLuxwFfDyBb4C6eWEhmahKPa30XEYlBQTW4uAH4ubuXA7j7zmDiBSszNYkLJwzkhWXb2F+t5hciEluCanAxEhhpZm+b2TwzO7e1jbqrwUV7LplURHVdIy8u2x6RzxcR6SpBNbhIAkYAZxBqdvEbM8ttuVF3Nbhoz6RBuRyT34tZ6mYkIjEmqAYXpcBz7l7n7huA1YQKfI9jZnxmYiHz1u9ly96qSMcREQlMUA0u/kLo7Bwzyyc0BLM+2KjBuXhiIQB/WVgW4SQiIsEJqsHFP4A9ZrYceB24zd17bO+34t4ZTBvam1kLyzQnXURiRlANLtzdb3X3Me5+vLs/2RVhg/SZSUVs2H2Q9zdrTrqIxIa4uVO0pfPG9ictOUEXR0UkZsRtQc9KS+bc4/rzt8VbqalviHQcEZGjFrcFHULDLvuq63l1RY+8D0pE5IjEdUE/ZXg+/bJTNewiIjEhrgt6YoJx8cRC5qzaxe4DNZGOIyJyVOK6oENoKYD6Rtc66SIS9eK+oI/sl8XxhTnMWqhhFxGJbnFf0AE+M6mQZWX7WLVdTaRFJHoF1uAivM0lZuZmNiWYeN3jwvEDSUowXRwVkagWSIMLADPLCm/z7tGG6m59MlM549i+PLuwjIZGLQUgItEpqAYXAD8E7gKqA8jV7S6ZVMjO/TW8tXZ3pKOIiHRKIA0uzGwSUOzuf29vJz2hwUVbzhrdl5z0ZA27iEjUOuoGF2aWANwPfPNw++oJDS7akpqUyKfHD+AfH2xXezoRiUpBNLjIAsYCc8LbTAOei7YLo/Cv9nR/X7It0lFERI7YUTe4cPdKd8939yHhbeYBF7p7SVeF7ioTinMZW5jN/3tppboZiUjUCarBRUwwMx68YhKNjc6Nv1/AoVqtwigi0SOQBhcttjkjGs/OmwzJ78UDV0xk5fZ93D5riToaiUjU0J2irTjz2L586xPH8tdFW3n4rQ2RjiMi0iEq6G348hnDOPe4/vzoxZXM1dx0EYkCKuhtMDPuvWw8x+T34itPLKS0XBdJRaRnU0FvR2ZqEg99YTJ19Y38x+MLqK7TRVIR6blU0A9jaEEmP7l8AsvK9vHdZ5fqIqmI9Fgq6B0wY3Q/vnH2SGa9X8bMuRsjHUdEpFUq6B10y1nDOXt0P3749xWUbNwb6TgiIh8TyHroZnarmS03syVm9qqZDQ42ZuQlJBj3f248A3PT+ObTi6mqrY90JBGRjwhqPfSFwBR3Hwf8Gbj7aIP1RNlpydxz6Xg27anirhdXRjqOiMhHBLIeuru/7u5N8/rmAUXBxOt5pg3tw7WnDGHmO5s0P11EepRA1kNv4TrgxU4nigLf/uQojsnvxW1/XqKldkWkxzjq9dBbbHsVMAW4p433e2yDiyORnpLIvZ8dz7bKQ/zfF9rsyici0q2CWA8dADM7G/geoaVza1rbUU9ucHGkJg/O44bThvLE/C3MWbUz0nFERI5+PXQAM5sI/JpQMY+b6vaNs0cyom8mtz+zlMpDGnoRkcgKaj30e4BM4GkzW2RmH1tWNxalJSdy/2UT2HWghv/+2weRjiMicS7pSDZ29znAnPDjO5u9fnagqaLI8UU53HzGMH762lrOGzuAc8b0i3QkEYlTulM0AF85awRjBmRzx6yllB+sjXQcEYlTKugBSElK4N7PjqfyUC3/9ddlWsBLRCJCBT0gYwZm87UZI3h+yTauffQ9Nu05GOlIIhJnVNAD9OUzhvNfF4zhvQ17OefHb/DAK2u0hrqIdBsV9AAlJBjXTT+GV795Bp8Y048fv7Ka8x54kzdWR+9NVCISPVTQu0D/nDQevHISv7/uBAC++Mh8bv7j+2yvrI5wMhGJZSroXejUEQW89PVT+eY5I3ll+Q5m3DeHR97aQGOjLpqKSPBU0LtYalIit8wYwexvnM4Jx/TmB88v50sz32OvpjeKSMCCanCRamZ/MrO1ZvaumQ0JMmQsGNQng0eumcr/XDyWuev2cP4Db/KeOh+JSICCanBxHVDu7sOBHwN3HW2wWGRmXDVtMLNuOpm05AQuf2gev5izVkMwIhKIQBpcABcBM8OP/wzMMDM7+nixaWxhDn+7ZTrnje3P3S+t4tpH32PPgVYXqBQR6bCgGlwUAlsA3L0eqAT6tNwoVtZDD0JWWjI/u2Ii/3PxWN5Zv4fzf/om8zdoCEZEOi/QBheHE0vroQehaQjm2S+fTEZKEpc/9A4PvraGBg3BiEgnBNXgogwoBjCzJCAH2BNgzph23MDQEMwF4wZy78urueq372rOuogcsUAaXADPAVeHH18a3kanmUcgMzWJBy6fwN2XjmPRlgrOe+ANXlm+I9KxRCSKBNXg4mGgj5mtBW4Fbg8iXLwxMy6bUszzX53OgJx0rn+shO8/94HWgxGRDrFInUhPmTLFS0pKIvLZ0aCmvoG7XlzFI29vYFT/LB68ciLD+2ZFOpaIRJiZLXD3Ka29pztFe6jUpETu/PQYfnfNVHbtr+GCn73FE/M3a611EWmTCnoPd+aovrz4tVOZMrg3d8xayi1PLORgTX2kY4lID6SCHgX6Zqfx2JdO4LZPHssLS7dx8c/fZt2uA5GOJSI9jAp6lEhIMG4+czi/v+5E9hys5aIH3+bFpdsiHUtEehAV9ChzyvB8nr9lOsP7ZnLTH97nRy+soL6hrRt4RSSeqKBHoYG56fzp36fxhWmD+fUb67nq4XfZtV9rwYjEOxX0KJWalMgPLx7LfZ8dz8LNFVzwszdZsKk80rFEJIJU0KPcJZOLePbLp5CalMjnfv0OP3t1DbX1GoIRiUcdWZwrzczmm9liM/vAzP67lW0Gmdnr4QYYS8zs/K6JK60ZMzCbv31lOueO7c99s1fzqZ+qeYZIPOrIGXoNcJa7jwcmAOea2bQW2/wn8JS7TyS03ssvgo0ph5OTkcyDV07id9dMpaq2gc/+6h3umLWUyqq6SEcTkW7SkcW53N2bJj0nh/+0vF3Rgezw4xxga2AJ5YicOaovs289jRtOPYY/vbeZGff/k+cWb9UdpiJxoENruZhZIrAAGA783N2/0+L9AcDLQB7QCzi7tfXTzexG4EaAQYMGTd60adNRfwPStmVllXz32aUsKa3k9JEF/M/FYynunRHpWCJyFNpby+WIFucys1zgWeAWd1/W7PVbw/u6z8xOIrT64lh3b/PqnBbn6h4Njc7MuRu57+VV1DU4YwuzGV+cy4TiXMYX5TK4TwZBdAusb2gkKVHX2EW6WmAFPbyzO4Eqd7+32WsfAOe6+5bw8/XANHff2dZ+VNC719aKQzw6dyMLN5eztKyS6rrQ/7U56cmML85lfFEO44pyGT0gi8Lc9MMWeXdnxbb9vLFmF2+s3kXJxnJOHNqbX101mV6pSd3xLYnEpfYK+mF/8sysAKhz9wozSwfOAe5qsdlmYAbwqJmNBtKA+G4a2sMMzE3nu+ePBkJn06t3HGBxaQWLt1SwuLSSn7++lqbOd1lpSYzun82oAVmM6p/N6AFZHNs/i0O1Dby1djf/XL2LN9fs/vBmplH9s7howkCeeb+Uqx+ZzyPXTiU7LTlS36pI3DrsGbqZjQNmAomELqI+5e4/MLMfACXu/pyZjQF+A2QSukD6bXd/ub396gy9Z6mqrWfFtn2s2Lafldv3sXLbflZu38+B8MqOTSfs7pCbkcz04fmcNrKA00YU0D8nDYAXlm7jq08sZMzAbB770gnkZqRE6tsRiVmBDrkERQW952tsdMoqDn1Y6BMMTh1ZwPGFOSQmtD4k8+qKHdz0+PsMLejF49efSH5majenFoltKujSrd5cs4sbHiuhKC+DP1x/Iv2y0yIdSSRmqGORdKtTRxTw6LUnsK3iEJf9+h1Ky6ta3a6yqo6/Lirjq08s5Mx753D7M0t4c80urR4p0kk6Q5cu8/7mcq5+ZD7Zacn88YYTGdynF+t3HeDVFTt5ZcUOSjaV09Do9OmVwtjCHEo27uVgbQO9e6Vw7tj+XDBuACce06fN4R2ReKQhF4mYZWWVfOHhd0lKTCArNYn1uw8CoZkxM0b35axR/ZhQnEtiglFd18CcVbt4fslWXl2xk0N1DeRnpnL+8f351PEDmDKkt4q7xD0VdImoVdv3862nF5PXK4WzR/flzGP7HvaO1UO1Dby2cifPL9nKayt3UlPfSO9eKZw1qi/njOnHaSMKSE9J7KbvQKTnUEGXqHawpp45q3Yxe/l2Xlu5k33V9aQmJXDqiHzOGdOPGaP7aTaNxA0VdIkZdQ2NzN+wl9nLdzB7+Q7KKg5hBtOH5/P1s0cyeXBepCOKdCkVdIlJ7s7ybft4+YMd/OHdTew+UMuMUX355ieOZczA7MPvQCQKHVVBN7M04A0gldBSAX929//TynaXAd8ndKfoYne/sr39qqBLkA7W1PPo3I38+p/r2FddzwXjBvCNc0YyrCAz0tFEAnW0Bd2AXu5+wMySgbeAr7n7vGbbjACeItQIo9zM+ra3MBeooEvXqDxUx2/eWM8jb2+guq6BSycX8dUZIyjK07LBEhuO6saiDja4uIHQOunl4b/TbjEX6So56cl865PH8sa3z+Sak4/hLwu3cta9/+SXc9ZFOppIl+vQnaJmlmhmi4CdwGx3f7fFJiOBkWb2tpnNM7Nz29jPjWZWYmYlu3ZpMUbpOvmZqdz56THMue0MZozuy10vreSnr66JdCyRLtWhgu7uDe4+ASgCTjCzsS02SQJGAGcAVwC/CTfDaLmfh9x9irtPKSgoOLrkIh0wMDedB6+cxGcmFXL/7NU8+JqKusSuI+pEEF4T/XXgXGBZs7dKgXfdvQ7YYGarCRX49wJLKtJJiQnGPZeOB4d7X16NmXHzmcMjHUskcEE1uPgLoTPz35lZPqEhmPVBhxXprMQE457PjseBe/6xCjP48hntF3V355UVO3lh6TZG9MvklGH5jG1n6WCRSOvIGfoAYGa4UXRTg4vnmze4AP4BfMLMlgMNwG3uvqfLUot0QmKCce9nx9Pozt0vrcIwbjpj2Me2a2h0Xli6jZ+/vpaV2/eTnZbEswvLgFVkpyUxbWgfThmezynD+zCsIDOQnqwiQdCNRRJ36hsaufWpxTy3eCt3nDeKfz89VNTrGhp5dmEZv5qzjvW7DzK8byY3nzmMT48bSHlVHXPX7Wbu2j28vW43peWHAOiblcqM0f343qdGk6leqtINjqqnqEisSUpM4P7LQsMvP3pxJQ3uZKUm8at/rqes4hBjBmTzy89P4pPH9SchPLxSkJXKRRMKuWhCIQCb91Tx9rrdvL12N0+VbGFJaQW/u3YqfbPUzEMiR2foErfqGxr5+p8W8fySbQBMGpTLLWeN4IxjC45oGOX1lTv58h/eJz8rhZnXnsBQ3Z0qXUhruYi0ob6hkUfnbmTMwGxOGtqn0+Phi7ZU8KVHQ5O6Hr56ChMHaZEw6RpqQSfShqTEBK4/dSgnD8s/qoubE4pzmXXTyWSmJnHFb+bx6oodAaYU6RgVdJGADMnvxTM3ncyIvlnc8FgJT87fHOlIEmdU0EUCVJCVypM3TmP6iAJun7WUB15ZQ6SGNSX+qKCLBKxXahIPXz2FSyYV8eNXVnPDYwt4ZkEpWysORTqaxDhNWxTpAsmJCdz72XEM6p3B7+Zu4JXwmPqQPhmcNKwP04b24aRhfTTNUQIVWIOL8LaXAH8Gprp7u1NYNMtF4kVjo7Ny+37mrtvNvPV7eHfDXvZX1wMwrKAXnziuP1dNG0xhbnqEk0o06PIGF+HtsoC/AynAV1TQRVrX0Oh8sLWSd9bt4e11e3hrTWgp6XPG9OPqk4Zw0rDOT5+U2HdUd4p6qOIfrsEFwA8JLdp1WydzisSFxARjXFEu44py+ffTh1FWcYjH523iyfmb+ccHOxjRN5MvnjyEz0wspFc7ywnUNzRysKaB7PQk/QcgQAdvLAovzLUAGE6oM9F3Wrw/Cfieu19iZnOAb7V2hm5mNwI3AgwaNGjypk2bjv47EIkR1XUNPL9kGzPnbmRpWSVZqUlcMrmI/MwUdh+oZfeBGvaEv+4+UEN5VR0A+ZkpHF+YE/5PIofji3I0Nh/DArtTNNy04lngFndfFn4tAXgNuMbdN7ZX0JvTkItI69ydhVsqmDl3Iy8s3UZdg5OVlkR+Zir5mSn06ZVKflboa6/URFbvOMCS0grW7jxAY/jHeUBOGscX5jB5cB4XTSikf44KfKwI9NZ/M7sTqHL3e8PPc4B1/GtYpj+wF7iwvaKugi5yeFW19SSYkZaceNhtD9bUs3zbPhZvqWBpWSVLSivZsPsgCQZnjerLFScM4vSRBSQlarZyNDuqMfTDNbhw90ogv9n2c+jAGbqIHF5GSsdnFvdKTWLqkN5MHdL7w9c27TnIk+9t4emSUl5ZUUL/7DQum1rM56YWa1ZNDOrILJdxwEygeYOLH7RocNF8+zloyEWkR6lraOTVFTt4Yv4W3gjPqjltRAFfmn4Mp49Uf99ootUWReRDW/ZW8XTJFv5UsoUd+2q48sRB/NenxpCecvhhHYk8FXQR+Zja+kbue3kVv35jPSP6ZvLTKyYyekB2l39uRVUtCzdXUFpxiLEDszluYA4pSRrX7ygVdBFp05trdnHrU4upPFTHd88bxdUnDwlsXntjo7Nm5wHe31zO+5vKWbC5nPW7Dn5km5SkBMYX5TBpUB6TBucxaVAeBVmpR/Q57s7WymoWbi5n4ebQReFBvTM4Z0w/Th2R3+FrERVVtby9dg8791dzxQmDOnQxurupoItIu3YfqOG2pxfz+qpdzBjVl7svHUefzLaLauWhOlZs28feg7UcqK5nf009B6rrOVBTx4GaevZX17P3YC1LSyvZXxNa5iAvI/kjRbsoL50PtlayYFM5CzaVs6xsH7UNjQAM7pPBmAHZ5PVKIS8jmbyMFPIyUujdK4Xc8POd+2s+LODvby5n5/4aAFKTEhg1IJsNuw6wr7qe1KQEpg/P5+wx/Zgxuu9H5ujX1jeycHM5b67ZzZtrdrGkrJKmkji+OJfffHFyj5vTr4IuIofl7jw6dyM/emEluRnJ3H/ZBKaPyGf3gRqWlVXywdZ9fLC1kmVl+9i8t6rVfaQnJ5KZlkRWahJZ6ckcNzA7VMQH5XJMfq92z/yr6xo+LPAlG8tZt+sAFVV1VByqo6Gx7To1uE8GE4tzmTgoj4mDchk9IJvkxATqGhp5b8NeZq/YwezlOygtP4RZqBnJSUP7sGr7ft5Zv4eq2gYSE4wJxbmcOiKfU0fks3NfDbc+tZi8jGQevmZqtwxFdZQKuoh02PKt+7jlifdZv/sgBZmpH575Qqh4Hhce9z5uYDb9c9LITE0iKzWZXqmJXTLHvbHR2V9dT3lV7b/+HKwjJz2ZiYNy2/1Nool7aIG0V5bvYPaKHSwprWRInwymj8jn1BEFnDSsD9lpyR/5O8vKKrl+Zgn7quv46eUTOXtMv0C+n7KKQyQnGH2zO3fmr4IuIkfkUG0DP3l1NTsqqxlbmMNxA3MYMzCbnPTkw//lKFBd19Ch8fEd+6q5fmYJy7ZW8r3zR3Pd9GM6dX3B3Zm3fi8z527k5eXbuebkY7jz02M6E10FXUSksw7VNnDrU4t4cdl2rjihmB9cNJbkDv4mUlVbz7MLy3hs7iZW7dhPbkYyl08dxFXTBlGUl9GpPEd1p6iISDxLT0nk51dO4r7Zq/j56+vYtKeKX35+MjkZbf+2snlPFY+9sxYVpUUAAAXWSURBVJGnSrawr7qe4wZmc/el47hw/MAunTnTkVv/D9vgwsxuBa4H6oFdwJfcXUspikhMSEgwbvvkKIbmZ3L7rCVM/d9XSE1OICnBSExo+mokJRoJZmzcc5BEM84d259rTh7C5MF53bLEcUfO0GuAs5o3uDCzF1s0uFgITHH3KjO7Cbgb+FwX5BURiZhLJhcxtKAXLy7bTl1DIw2NTn2j09AQ+lrf2Eh9g3Ph+IFceeIg+nXywmdnBdLgwt1fb/Z0HnBVUAFFRHqS0PTIvEjHaFWHRvbNLNHMFgE7gdnu/m47m18HvNjGfm40sxIzK9m1a9eRpxURkTZ1qKC7e4O7TwCKgBPMbGxr25nZVcAU4J429vOQu09x9ykFBVrhTUQkSEd0F4C7VwCvA+e2fM/Mzga+R6ixRU3L90VEpGsdtqCbWUG49RzNGlysbLHNRODXhIr5zq4IKiIi7evILJcBwMxwo+imBhfPt2hwcQ+QCTwdnpqz2d0v7KrQIiLycR2Z5bIEmNjK63c2e3x2wLlEROQIaVV5EZEYoYIuIhIjIrY4l5ntAjq7PEA+sDvAOEFSts7pydmgZ+dTts6J1myD3b3Ved8RK+hHw8xK2lptLNKUrXN6cjbo2fmUrXNiMZuGXEREYoQKuohIjIjWgv5QpAO0Q9k6pydng56dT9k6J+ayReUYuoiIfFy0nqGLiEgLKugiIjEi6gq6mZ1rZqvMbK2Z3R7pPM2Z2UYzW2pmi8wsoh2wzewRM9tpZsuavdbbzGab2Zrw14is0t9Gtu+bWVn42C0ys/MjlK3YzF43s+Vm9oGZfS38esSPXTvZIn7szCzNzOab2eJwtv8Ov36Mmb0b/nn9k5ml9KBsj5rZhmbHbUJ3Z2uWMdHMFprZ8+HnnTtu7h41f4BEYB0wFEgBFgNjIp2rWb6NQH6kc4SznAZMApY1e+1u4Pbw49uBu3pQtu8D3+oBx20AMCn8OAtYDYzpCceunWwRP3aAAZnhx8nAu8A04Cng8vDrvwJu6kHZHgUujfS/uXCuW4E/As+Hn3fquEXbGfoJwFp3X+/utcCTwEURztQjufsbwN4WL18EzAw/nglc3K2hwtrI1iO4+zZ3fz/8eD+wAiikBxy7drJFnIe01qryLODP4dcjddzaytYjmFkR8Cngt+HnRiePW7QV9EJgS7PnpfSQf9BhDrxsZgvM7MZIh2lFP3ffFn68HegXyTCt+IqZLQkPyUS8aaOZDSG00ui79LBj1yIb9IBj17JVJaHfpivcvT68ScR+Xttpo/m/4eP2YzNLjUQ24CfAt4HG8PM+dPK4RVtB7+mmu/sk4DzgZjM7LdKB2uKh3+V6zFkK8EtgGDAB2AbcF8kwZpYJPAN83d33NX8v0seulWw94th5i1aVwKhI5GhNy2zhNpp3EMo4FegNfKe7c5nZBcBOd18QxP6iraCXAcXNnheFX+sR3L0s/HUn8Cyhf9Q9yQ4zGwAQ/tpjuku5+47wD10j8BsieOzMLJlQwfyDu88Kv9wjjl1r2XrSsQvnaWpVeRKQa2ZNfRci/vPaLNu54SEs91DLzN8RmeN2CnChmW0kNIR8FvAAnTxu0VbQ3wNGhK8ApwCXA89FOBMAZtbLzLKaHgOfAJa1/7e63XPA1eHHVwN/jWCWj2gqlmH/RoSOXXj88mFghbvf3+ytiB+7trL1hGNnrbeqXEGoeF4a3ixSx63VNprN/oM2QmPU3X7c3P0Ody9y9yGE6tlr7v55OnvcIn11txNXg88ndHV/HfC9SOdplmsooVk3i4EPIp0NeILQr991hMbgriM0NvcqsAZ4Bejdg7L9HlgKLCFUPAdEKNt0QsMpS4BF4T/n94Rj1062iB87YBywMJxhGXBn+PWhwHxgLfA0kNqDsr0WPm7LgMcJz4SJ1B/gDP41y6VTx023/ouIxIhoG3IREZE2qKCLiMQIFXQRkRihgi4iEiNU0EVEYoQKuohIjFBBFxGJEf8fU1LrpfYIHE8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJOvbCaWWGyH"
      },
      "source": [
        "# **Evaluation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfN_tC5Y_bev"
      },
      "source": [
        "def test(test_pairs, encoder, decoder, loss_fxn, max_length = MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "\n",
        "        for i in range(len(test_pairs)):\n",
        "            input_tensor = test_pairs[i][0]\n",
        "            target_tensor = test_pairs[i][1]\n",
        "\n",
        "            input_length = input_tensor.size(0)\n",
        "            target_length = target_tensor.size(0)\n",
        "\n",
        "            encoder_hidden = encoder.initHidden()\n",
        "            encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
        "\n",
        "            loss = 0\n",
        "            for ei in range(min(input_length, max_length)):\n",
        "                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "                encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "            decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze().detach()\n",
        "\n",
        "                loss +=  loss_fxn(decoder_output, target_tensor[di])\n",
        "                if decoder_input.item() == EOS_token:\n",
        "                    break\n",
        "                    \n",
        "        test_loss += loss.item()/target_length\n",
        "\n",
        "    return test_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUEBi7yt_s4B",
        "outputId": "e46585c3-8112-4a3b-bcda-ed7fc026ca9d"
      },
      "source": [
        "test(test_pairs, encoder, decoder, LOSS_FXN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.036038208007813"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo4qiySVLo3L"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size(0)\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
        "\n",
        "        for ei in range(min(input_length, max_length)):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            if topi.item() == EOS_token:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2d76fJvLyJI"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n = 100):\n",
        "    total_bleu_scores = 0\n",
        "    total_meteor_scores = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('INPUT: ', pair[0])\n",
        "        print('TARGET: ', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('PRED: ', output_sentence)\n",
        "        total_bleu_scores += sentence_bleu(pair[1].split(\" \"), output_sentence.split(\" \"))\n",
        "        total_meteor_scores += single_meteor_score(pair[1], output_sentence)\n",
        "        print('')\n",
        "\n",
        "    bleu_result = total_bleu_scores/n\n",
        "    meteor_result = total_meteor_scores/n\n",
        "    print(\"\\n\\nbleu score: \",bleu_result)\n",
        "    print(\"meteor score: \",meteor_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMvLoCOoLyFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a516328-f4e8-486e-989b-4c9b072e9885"
      },
      "source": [
        "evaluateRandomly(encoder, decoder, n = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT:  लेकिन यह मेरे दिमाग में अप्रमाणित है.\n",
            "TARGET:  But this is unproven in my mind.\n",
            "PRED:  But this is a very important part of me .\n",
            "\n",
            "INPUT:  जब आप कोबरा कमांडर को खोजने के लिए.\n",
            "TARGET:  For when you find Cobra Commander.\n",
            "PRED:  When you find the rest of the old man for you .\n",
            "\n",
            "INPUT:  फिल, वे मेरे क्रेडिट है कार्ड नीचे.\n",
            "TARGET:  Phil, they have my credit card downstairs.\n",
            "PRED:  Phil , they 've got a card 's card .\n",
            "\n",
            "INPUT:  प्रशिक्षण के दौरान, मैं अपने दुश्मन की रणनीति तैयार करेंगे.\n",
            "TARGET:  During training, I will devise the strategy of your enemy.\n",
            "PRED:  For the enemy , I 'll be ready to the the .\n",
            "\n",
            "INPUT:  मैं होगा आप ढीला काटा.\n",
            "TARGET:  I gotta cut you loose.\n",
            "PRED:  I will be able to you .\n",
            "\n",
            "INPUT:  लेकिन मैं चाहूंगा कि आप सिकुड जायें लगभग १००० गुना, उस अनुपात में जहां पर मनुष्य के एक बाल मेरे हाथ जितना बडा हो।\n",
            "TARGET:  But I just need to shrink you by a factor of 1000, to a scale where the diameter of a human hair is as big as my hand.\n",
            "PRED:  But I 'm a important moment to get the to the to the the to the the to the the to the the to the the to the the to the the to the the to the the to the the to the the to the the to the the to the the to the the to the the .\n",
            "\n",
            "INPUT:  औरत [टीवी पर]: रदरफोर्ड उम्रकैद की सजा काट रहा था... ...एक 1968 शूटिंग के लिए.\n",
            "TARGET:  Rutherford was serving a life sentence for a 1968 shooting.\n",
            "PRED:  [ man ON SPEAKERS ] man :\n",
            "\n",
            "INPUT:  -धत !\n",
            "TARGET:  Shit!\n",
            "PRED:  - Damn , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no , no\n",
            "\n",
            "INPUT:  15 साल के लिए, एंटोन और मैं एक छोटी दिखाने बुलाया थाः\n",
            "TARGET:  For 15 years, Anton and I did a little show called:\n",
            "PRED:  For a year , I had a man to the old man .\n",
            "\n",
            "INPUT:  मैं तीस साल की थी, जब एक फोन कॉल आया मेरे डॉक्टर के कार्यालय से मेरे परीक्षण के परिणाम बताने के लिए ।\n",
            "TARGET:  I was 30 years old, and I received a call from my doctor's office to say my test results were in.\n",
            "PRED:  I was a call for a doctor call to a doctor 's call a year - off - year - old phone .\n",
            "\n",
            "\n",
            "\n",
            "bleu score:  9.062576657159591e-231\n",
            "meteor score:  1.6143063279164909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T33hIoTthTG"
      },
      "source": [
        "# **Submission**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXatBjRvYtAD"
      },
      "source": [
        "test = pd.read_csv(\"test_week2.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqzeMhxcVFN5"
      },
      "source": [
        "def evaluateTest():\n",
        "    encoder.load_state_dict(torch.load(\"encoder_bestTrain.pt\"))\n",
        "    decoder.load_state_dict(torch.load(\"decoder_bestTrain.pt\"))\n",
        "    file1 = open(\"week2_5.txt\", 'w')\n",
        "    for i, data in test.iterrows():\n",
        "        sentence = data[\"hindi\"]\n",
        "        output_words = evaluate(encoder, decoder, sentence)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        file1.writelines(output_sentence + \" \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN_KjB9mnfmK"
      },
      "source": [
        "evaluateTest()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOXyU9OgrRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7a32c3-8763-4795-e85f-bf765d20b5e1"
      },
      "source": [
        "file2 = open(\"week2_5.txt\", 'r')\n",
        "ref = file2.readlines()\n",
        "ref[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Who knows they 've been a secret to your heart ? \\n\",\n",
              " \"We 're the more likely to get the more . \\n\",\n",
              " 'Then they made me . \\n',\n",
              " '- Yeah , how did we ? \\n',\n",
              " 'No way . \\n',\n",
              " \"And the reason why we 're always having to do this together , the way that the best way of the United States , the way that the United States , the way that the best way you have the story , the more important to the story , the more important to the story , the more important to the more important to the end of the world , the more important to the more than that the planet is not to be able to get to the best way of you . \\n\",\n",
              " \"I 'm never going to be happy to do it . \\n\",\n",
              " \"And we 're going to get a lot of the best to the best of the best ones that we 've done the best of the best way of the planet . \\n\",\n",
              " \"( Laughter ) So we have n't always had the idea that the idea is a true of the idea that the best reason is to have the best of the best of the planet that we have the best of the best of the best of the world that we have to do with the rest of the world . \\n\",\n",
              " \"They shoot 'em ! \\n\",\n",
              " \"I 'd like to give him a few years ago to get used to the first . \\n\",\n",
              " \"( Laughter ) Because the same thing is so interesting to the whole thing that 's it 's about . \\n\",\n",
              " 'He was a one of the one was a one of the one . \\n',\n",
              " 'First is a first day . \\n',\n",
              " 'We need you to take the whole . \\n',\n",
              " 'He was able to get if you get paid for me . \\n',\n",
              " 'So this will be a way of you to reach the last few years . \\n',\n",
              " 'I had a word . \\n',\n",
              " \"There 's a hundred percent for you , there is no way to get back the same . \\n\",\n",
              " 'A curse . \\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VepMHz8YV9qx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}